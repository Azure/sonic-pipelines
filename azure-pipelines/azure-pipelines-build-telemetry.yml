# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger: none
pr: none

schedules:
- cron: "0 */2 * * *"
  displayName: Hourly build
  branches:
    include:
    - main
  always: true

name: $(TeamProject)_$(Build.DefinitionName)_$(SourceBranchName)_$(Date:yyyyMMdd)$(Rev:.r)

stages:
- stage: Build
  pool: sonicbld
  jobs:
  - job: Build
    timeoutInMinutes: 120
    steps:
    - script: |
        sudo apt-get update
        sudo apt-get install -y python3-pip
        sudo pip3 install azure-storage-queue azure-storage-blob
        sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 10
      displayName: Install build tools
    - task: PythonScript@0
      displayName: Publish SONiC telemetry
      inputs:
        scriptSource: 'inline'
        script: |
          import datetime, base64, json, time, os, re
          from urllib import request
          from azure.storage.queue import QueueClient
          from azure.storage.blob import BlobServiceClient

          QUEUE_NAME="builds"
          CONTAINER="build"
          if os.getenv('AZURE_STORAGE_QUEUE_NAME'):
            QUEUE_NAME = os.getenv('AZURE_STORAGE_QUEUE_NAME')
          if os.getenv('AZURE_STORAGE_CONTAINER'):
            CONTAINER = os.getenv('AZURE_STORAGE_CONTAINER')
          print("QUEUE_NAME={} AZURE_STORAGE_CONTAINER={}".format(QUEUE_NAME, CONTAINER))
          AZURE_STORAGE_CONNECTION_STRING='$(AZURE_STORAGE_CONNECTION_STRING)'
          BUILD_MESSAGES = 'buildmessages'
          BUILD_INFOS = 'builds'
          BUILD_LOGS = 'buildlogs'
          BUILD_COVERAGES = 'buildcoverages'
          MESSAGE_PER_PAGE = 10
          MAX_PAGE_COUNT = 30
          HEADERS = {"Authorization": "Bearer " + "$(System.AccessToken)"}
          blob_service_client = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)

          # Upload a list of lines to blob
          def upload_to_blob(lines, blob_prefix, file_prefix=""):
            now = datetime.datetime.now()
            if len(lines) == 0:
              return
            local_file_name = file_prefix + now.strftime("_%Y%m%d-%H%M%S-%f") + '.json'
            with open(local_file_name, "w") as file:
              count = file.write('\n'.join(lines))
            blob_file_name = blob_prefix + now.strftime("/%Y/%m/%d/") + local_file_name
            blob_client = blob_service_client.get_blob_client(container=CONTAINER, blob=blob_file_name)
            with open(local_file_name, "rb") as data:
              blob_client.upload_blob(data)
            os.remove(local_file_name)

          # Download the web content from the url
          def get_response(url):
              for i in range(0, 3):
                  try:
                    print(url)
                    req = request.Request(url, headers=HEADERS)
                    response = request.urlopen(req, timeout=300)
                    data=response.read()
                    encoding = response.info().get_content_charset('utf-8')
                    return data.decode(encoding)
                  except Exception as e:
                    print(e)
                    time.sleep(10)
              raise Exception('failed to get response from {0}'.format(url))

          def get_coverage(build_info):
              base_url = re.sub('/_apis/.*', '/_apis', build_info['url'])
              url = '{0}/test/codecoverage?buildId={1}&api-version=6.0-preview.1'.format(base_url, build_info['id'])
              coverage_content = get_response(url)
              info = json.loads(json.dumps(build_info))
              coverage = json.loads(coverage_content)
              results = []
              if 'coverageData' in coverage and len(coverage['coverageData']) > 0:
                  info['coverage'] = coverage_content
                  results.append(json.dumps(info))
              return results

          # Get the build logs
          def get_build_logs(timeline_url, build_info):
              timeline_content =  get_response(timeline_url)
              if not timeline_content:
                  return []
              records = json.loads(timeline_content)['records']
              results = []
              #max_column_size = 104855000
              max_column_size = 40*1024*1024 #40M
              for record in records:
                  record['content'] = ""
                  record['buildId'] = build_info['id']
                  record['definitionId'] = build_info['definitionId']
                  record['definitionName'] = build_info['definitionName']
                  record['sourceBranch'] = build_info['sourceBranch']
                  record['sourceVersion'] = build_info['sourceVersion']
                  record['triggerInfo'] = build_info['triggerInfo']
                  record['reason'] = build_info['reason']
                  if record['log']:
                      log_url = record['log']['url']
                      log = get_response(log_url)
                      content = log[:max_column_size]
                      lines = []
                      for line in content.split('\n'):
                          if '&sp=' in line and '&sig=' in line:
                              continue
                          lines.append(line)
                      record['content'] = '\n'.join(lines)
                  if 'parameters' in build_info:
                      record['parameters'] = build_info['parameters']
                  if 'status' in build_info:
                      record['status'] = build_info['status']
                  if 'uri' in build_info:
                      record['uri'] = build_info['uri']
                  results.append(json.dumps(record))
              return results

          queue_client = QueueClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING, QUEUE_NAME)
          messages = queue_client.receive_messages(messages_per_page=MESSAGE_PER_PAGE, visibility_timeout=3600)
          page = 0
          for msg_batch in messages.by_page():
              page = page + 1
              if page > MAX_PAGE_COUNT:
                  break
              local_file_name = datetime.datetime.now().strftime("_%Y%m%d-%H%M%S-%f") + '.json'
              build_messages = []
              msgs = []
              build_infos = []
              build_logs = []
              build_coverages = []
              msg_count = 0
              for msg in msg_batch:
                  msg_count = msg_count + 1
                  print("process message {} on page {}, current log count {}".format(msg_count, page, len(build_logs)))
                  msgs.append(msg)
                  msg_content = base64.b64decode(msg.content)
                  build = json.loads(msg_content)
                  content = json.dumps(build, separators=(',', ':'))
                  build_messages.append(content)
                  build_url = build['resource']['url']
                  if 'dev.azure.com' not in build_url:
                    print("Skipped the the url {}".format(build_url))
                    continue
                  build_content = get_response(build_url)
                  if not build_content:
                      print("Skipped the message for no build content, the message: {}".format(msg_content))
                      continue
                  build_info = json.loads(build_content)
                  build_info['definitionId'] = build_info['definition']['id']
                  build_info['definitionName'] = build_info['definition']['name']
                  build_infos.append(json.dumps(build_info))
                  timeline_url = build_info['_links']['timeline']['href']
                  logs = get_build_logs(timeline_url, build_info)
                  build_logs += logs
                  build_coverages += get_coverage(build_info)
              upload_to_blob(build_messages, BUILD_MESSAGES)
              upload_to_blob(build_infos, BUILD_INFOS)
              upload_to_blob(build_coverages, BUILD_COVERAGES)
              split_build_logs = []
              log_size = 0
              max_upload_size = 80 * 1024 * 1024 # 80M
              for build_log in build_logs:
                  if log_size >= max_upload_size:
                      print("Split the logs to upload, log_size {}".format(log_size))
                      upload_to_blob(split_build_logs, BUILD_LOGS)
                      split_build_logs = []
                      log_size = 0
                  split_build_logs.append(build_log)
                  log_size += len(build_log)
              print("Upload log, log_size {}".format(log_size))
              upload_to_blob(split_build_logs, BUILD_LOGS)
              for msg in msgs:
                  queue_client.delete_message(msg)
          exit(0)
      env:
        AZURE_STORAGE_CONNECTION_STRING: '$(AZURE_STORAGE_CONNECTION_STRING)'
